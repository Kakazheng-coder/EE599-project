# -*- coding: utf-8 -*-
"""SGLA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dt1CKdkkSfP3gskVB--62HpSvI2NPOD2
"""

#coding=utf-8

from __future__ import absolute_import
from __future__ import division
from SENET import senet154
import torch
from torch import nn
from torch.nn import functional as F
import torchvision


class Spatial(nn.Module):
  def __init__(self):
    super(Spatial, self).__init__()
    self.conv1 = nn.Conv2d(1, 1, kernel_size=3, stride=2, padding=1)
    self.bn1 = nn.BatchNorm2d(1)
    self.conv2 = nn.Conv2d(1, 1, kernel_size=1)
    self.bn2 = nn.BatchNorm2d(1)
  def forward(self, x):
    # global cross-channel averaging
    x = torch.mean(x,1, keepdim=True) # 由hwc 变为 hw1
    # 3-by-3 conv
    h = x.size(2)
    x = F.relu(self.bn1(self.conv1(x)))
    # bilinear resizing
    x = F.upsample(x, (h,h), mode='bilinear', align_corners=True)
    # scaling conv
    x = F.relu(self.bn2(self.conv2(x)))
    return x

class Channel(nn.Module):
    
  def __init__(self, c, r=16):
    super(Channel, self).__init__()
    self.conv1 = nn.Conv2d(c, c // r, 1)
    self.bn1 = nn.BatchNorm2d(c // r)
    self.conv2 = nn.Conv2d(c // r, c, 1)
    self.bn2 = nn.BatchNorm2d(c)


  def forward(self, x):
    # squeeze operation (global average pooling)
    x = F.avg_pool2d(x, x.size()[2:]) #输出是1*1*c
    # excitation operation (2 conv layers)
    x = F.relu(self.bn1(self.conv1(x)))
    x = F.relu(self.bn2(self.conv2(x)))
    return x 

class SCA(nn.Module):
    
  def __init__(self,c):
    super(SCA,self).__init__()
    self.S = Spatial()
    self.C = Channel(c,16)
    self.conv = nn.Conv2d(c,c, kernel_size=1)
    self.bn1 = nn.BatchNorm2d(c)
    
  def forward(self,x):
    S = self.S(x)
    C = self.C(x)
    SC = S*C

    return torch.sigmoid(F.relu(self.bn1(self.conv(SC))))

class Hard(nn.Module):
    
  def __init__(self, in_channels):
    super(Hard, self).__init__()
    self.fc = nn.Linear(in_channels, 4*2)
    self.init_params()

  def init_params(self):
    self.fc.weight.data.zero_()
    self.fc.bias.data.copy_(torch.tensor([0.3, -0.3, 0.3, 0.3, -0.3, 0.3, -0.3, -0.3], dtype=torch.float))

  def forward(self, x):
    # squeeze operation (global average pooling)
    x = F.avg_pool2d(x, x.size()[2:]).view(x.size(0), x.size(1))
    # predict transformation parameters
    theta = torch.tanh(self.fc(x))
    theta = theta.view(-1, 4, 2)
    return theta

class ConvBlock(nn.Module):
  """Basic convolutional block.

  convolution + batch normalization + relu.
  Args:
      in_c (int): number of input channels.
      out_c (int): number of output channels.
      k (int or tuple): kernel size.
      s (int or tuple): stride.
      p (int or tuple): padding.
  """
  def __init__(self, in_c, out_c, k, s=1, p=0):
    super(ConvBlock, self).__init__()
    self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)
    self.bn = nn.BatchNorm2d(out_c)

  def forward(self, x):
    return F.relu(self.bn(self.conv(x)))

class Inception(nn.Module):

    def __init__(self, in_channels, out_channels):
        super(Inception, self).__init__()
        mid_channels = out_channels // 4

        self.stream1 = nn.Sequential(
            ConvBlock(in_channels, mid_channels, 1),
            ConvBlock(mid_channels, mid_channels, 3, s=2, p=1),
        )
        self.stream2 = nn.Sequential(
            ConvBlock(in_channels, mid_channels, 1),
            ConvBlock(mid_channels, mid_channels, 3, p=1),
            ConvBlock(mid_channels, mid_channels, 3, s=2, p=1),
        )
        self.stream3 = nn.Sequential(
            nn.MaxPool2d(3, stride=2, padding=1),
            ConvBlock(in_channels, mid_channels*2, 1),
        )

    def forward(self, x):
        s1 = self.stream1(x)
        s2 = self.stream2(x)
        s3 = self.stream3(x)
        y = torch.cat([s1, s2, s3], dim=1)
        return y
    
class GlobalNetwork(nn.Module):

  def __init__(self):
    super(GlobalNetwork, self).__init__()
    # Setting up the Sequential of CNN Layers
    
    self.senet154_ = senet154(num_classes=251, pretrained=None)

    self.layer0 = self.senet154_.layer0
    #global backbone
    self.layer1 = self.senet154_.layer1
    self.layer2 = self.senet154_.layer2
    self.layer3 = self.senet154_.layer3
    self.layer4 = self.senet154_.layer4

    self.SC1 = SCA(512)
    self.SC2 = SCA(1024)
    self.SC3 = SCA(2048)

  def forward(self, x):
    batch_size = x.size()[0]  # obtain the batch size
    x0 = self.layer0(x)
    x1 = self.layer1(x0)
    x2 = self.layer2(x1)
    A2 = self.SC1(x2)
    x2_out = x2*A2

    #GAP1 = F.adaptive_avg_pool2d(x2_out, (1, 1)).view(x2_out.size(0), -1)
    
    x3 = self.layer3(x2_out)
    A3 = self.SC2(x3)
    x3_out = x3*A3

    #GAP2 = F.adaptive_avg_pool2d(x3_out, (1, 1)).view(x3_out.size(0), -1)

    x4 = self.layer4(x3_out)
    A4 = self.SC3(x4)
    x4_out = x4*A4

   # x4_avg = F.avg_pool2d(x4_out, x4_out.size()[2:]).view(x4_out.size(0),  -1)

    #concat = torch.cat([GAP1,GAP2,x4_avg],1)

    return x2_out, x2, x3_out, x3, x4_out, x4

class LocalNetwork(nn.Module):
  def __init__(self):
    super(LocalNetwork,self).__init__()

    self.scale_factors = [torch.tensor([[0.5, 0], [0, 0.5]]) for i in range(4)]
    self.hard1 = Hard(512)
    self.hard2 = Hard(1024)
    self.hard3 = Hard(2048)
    
    self.local_fc = nn.Sequential(
                        nn.Linear(2048+512+1024, 2048),
                        nn.BatchNorm1d(2048),
                        nn.ReLU(),
                        )
    self.classifier_local = nn.Linear(2048,3)
    self.dropout = nn.Dropout(0.2)

  def stn(self,x, theta):
    """Performs spatial transform
    
    x: (batch, channel, height, width)
    theta: (batch, 2, 3)
    """
    grid = F.affine_grid(theta, x.size())
    grid = grid.cuda()
    x = F.grid_sample(x, grid)
    return x.cuda()

  def transform_theta(self,theta_i, region_idx):
    """Transforms theta to include (s_w, s_h), resulting in (batch, 2, 3)"""
    scale_factors = self.scale_factors[region_idx]
    theta = torch.zeros(theta_i.size(0), 2, 3)
    theta[:,:,:2] = scale_factors
    theta[:,:,-1] = theta_i
    #theta = theta.cuda()
    return theta

  def local(self,x,x_theta,in_c,out_c):
    x_local_list = []

    for region_idx in range(4):
      x_theta_i = x_theta[:,region_idx,:]
      x_theta_i = self.transform_theta(x_theta_i, region_idx)
      x_trans_i = self.stn(x, x_theta_i)  #256*56*26
      x_trans_i = F.upsample(x_trans_i, (56, 56), mode='bilinear', align_corners=True)
      x_local_i = x_trans_i
      local_conv = Inception(in_c,out_c)
      x_local_i =  local_conv(x_local_i)
      x_local_list.append(x_local_i)
    return x_local_list 


  def forward(self, x2,x3,x4):
    x2_hard = self.hard1(x2)
    x3_hard = self.hard2(x3)
    x4_hard = self.hard3(x4)
    x2_local_list = self.local(x2,x2_hard,512,512)
    x3_local_list = self.local(x3,x3_hard,1024,1024)
    x4_local_list = self.local(x4,x4_hard,2048,2048)

    x_local_list = []

    local1= torch.randn(16, 4, 512).cuda()
    local2 = torch.randn(16, 4, 1024).cuda()
    local3 = torch.randn(16, 4, 2048).cuda()

    for region_idx in range(4):

      x2_local_i = x2_local_list[region_idx]
      x2_local_i = F.avg_pool2d(x2_local_i, x2_local_i.size()[2:]).view(x2_local_i.size(0), -1) 
      local1[:,region_idx] = x2_local_i

      x3_local_i = x3_local_list[region_idx]
      x3_local_i = F.avg_pool2d(x3_local_i, x3_local_i.size()[2:]).view(x3_local_i.size(0), -1) 
      local2[:,region_idx] = x3_local_i


      x4_local_i = x4_local_list[region_idx]
      x4_local_i = F.avg_pool2d(x4_local_i, x4_local_i.size()[2:]).view(x4_local_i.size(0), -1) 
      local3[:,region_idx] = x4_local_i

    local1_max = local1.max(1)[0]
    local2_max = local2.max(1)[0]
    local3_max = local3.max(1)[0]
    local_concate = torch.cat([local1_max, local2_max, local3_max], 1)
    local_fc = self.local_fc(local_concate)
    local_out = self.classifier_local(self.dropout(local_fc))

    return local_fc, local_out

class SGLANetwork(nn.Module):
    def __init__(self):
        self.global = GlobalNetwork()
        self.local = LocalNetwork()
        self.dropout = nn.Dropout(0.2)
        
        self.global_fc = nn.Sequential(
                                nn.Linear(2048+512+1024, 2048), # 将4个区域 融合成一个 需要加上batchnorma1d, 和 relu
                                nn.BatchNorm1d(2048),
                                nn.ReLU(),
                                )

        self.global_out = nn.Linear(2048,num_classes)
   
    def forward(self, x):
        x = x.cuda()
        x2_out, x2, x3_out, x3, x4_out, x4 = self.global(x)
        GAP1 = F.adaptive_avg_pool2d(x2_out, (1, 1)).view(x2_out.size(0), -1)
        GAP2 = F.adaptive_avg_pool2d(x3_out, (1, 1)).view(x3_out.size(0), -1)
        x4_avg = F.avg_pool2d(x4_out, x4_out.size()[2:]).view(x4_out.size(0),  -1)
        
        concat = torch.cat([GAP1,GAP2,x4_avg],1)
        global_fc = self.global_fc(multi_scale_feature)
        global_out = self.global_out(self.dropout(global_fc))
        
        local_fc, local_out = self.local(x2,x3,x4)
        
        out = torch.cat([global_fc,local_fc],1)
        
        return out, global_out, local_out
        
        
        
